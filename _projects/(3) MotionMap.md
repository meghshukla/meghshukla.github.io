---
name: "MotionMap: Representing Multimodality in Human Pose Forecasting"
slug: motionmap
tools: [Application]
image: https://raw.githubusercontent.com/deep-regression/deep-regression.github.io/master/files/images/motionmap.png
description: '<img src="https://na.eventscloud.com/file_uploads/33157a7e4788e837751f71d7fa44a8b7_CVPR_Nashville_FinalLogo.jpg" alt="CVPR Logo" style="width: 100px; height: auto;">'
---

<style>
.responsive-video {
  width: 100%;
  height: auto;
  max-width: 100%;
}
</style>


# MotionMap: Representing Multimodality in Human Pose Forecasting
<br><br>
<span style="font-size:20px;">**Reyhaneh Hosseininejad\* and Megh Shukla\***, <br> Saeed Saadatnejad, Mathieu Salzmann, Alexandre Alahi  <br>
*Computer Vision and Pattern Recognition (CVPR) 2025*</span>

<a href="https://arxiv.org/pdf/2412.18883"><img alt="arXiv" src="https://img.shields.io/badge/CVPR%202025-OpenReview%20(link)-black?style=flat&logo=data%3Aimage%2Fjpeg%3Bbase64%2C%2F9j%2F4AAQSkZJRgABAQAAAQABAAD%2F2wBDAAoHBwgHBgoICAgLCgoLDhgQDg0NDh0VFhEYIx8lJCIfIiEmKzcvJik0KSEiMEExNDk7Pj4%2BJS5ESUM8SDc9Pjv%2F2wBDAQoLCw4NDhwQEBw7KCIoOzs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozs7Ozv%2FwAARCAEiAcIDASIAAhEBAxEB%2F8QAGwABAAIDAQEAAAAAAAAAAAAAAAYHAQQFAwL%2FxABDEAABAwICAwoMBQQDAAMAAAAAAQIDBAUGETFBURIVISJhgZGTstETFBYjNTZUcXJzscEyUlWCoUJikvAkM0NEU2P%2FxAAZAQEAAwEBAAAAAAAAAAAAAAAAAwQFAgH%2FxAAnEQEAAgIBAwQDAQEBAQAAAAAAAQIDERIEMlETFCExIjNBcWEjQv%2FaAAwDAQACEQMRAD8AuYAAAAAAAAAAADymqIaaNZJ5WRsTS565IB6gjlbjKihzbSRvqXfm%2FC3pXh%2Fg4dTiy61GaMkZTtXVG3h6VJq4L2RzlrCfHk%2Bqp4%2FxzxN%2BJ6IVnNW1dQuc1VNJ8T1U18k2ITR0vmUfr%2BIWhvlQe20%2FWt7z7bW0r%2BBlTC73SIpVmSbE6BkmxOg99rHl568%2BFsIqKnAuach9FVRVE8C5wzSRqn5Hqh0abE12pv8A5PhW7JWo7%2BdJxPS2%2Fkuozx%2FYWICK0eNYnKja2mdH%2FfEu6To0kgpK%2Bkr493SzslTXuV4U96aUILY7V%2B4SxetvptAwhk4dAAAAAAAAAAAAAAAAAAAHlLPFTs3c0rI255ZvciJ%2FJ6kcxr6Kh%2BenZU6pXlaIc2nUbdjfKg9up%2Btb3md8qD26n61veVfkmxBkmxC57WPKD158LQ3yoPbqfrW943yoPbqfrW95V%2BSbEGSbEHtY8nrz4WhvlQe3U%2FWt7xvlQe3U%2FWt7yr8k2IMk2IPax5PXnwtDfKg9up%2Btb3jfKg9up%2Btb3lX5JsQZJsQe1jyevPhaG%2BVB7dT9a3vG%2BVB7dT9a3vKvyTYgyTYg9rHk9efC0N8qD26n61veN8qD26n61veVfkmxBkmxB7WPJ68%2BFob5UHt1P1re8b5UHt1P1re8q%2FJNiDJNiD2seT158LQ3yoPbqfrW95jfKg9up%2Btb3lYZJsQZJsQe1jyevPha0csczEkie17F0OauaKfZx8K%2Br8Hvd2lOwhTtHGZhYrO42yADl6AAAAAABgDJ8ue1jVc5yNaiZqqrkiGvX3Cnt1Ms9TJuWpoTW5diJrIJd79VXZ6tcqxU6LxYkXTyrtUlx4rX%2FwAR3vFXduuMI4t1DbmpK%2FQsrvwp7k1kUqqyprpfC1Uz5Xat0vAnuTUeINCmOtPpVtebfYACRwAAAAAAAAH1FLJDIkkUjo3t0OauSofIAk9rxhLEqRXFvhGf%2FaxOMnvTWSymqoauFJqeVskbtDmqVYbdvudVbJ%2FC00mWf4mLwtf70KuTp4n5r8J6ZZj4lZpk0LPdI7tR%2BHZG6NUXcua5OBF5F1m%2BUZiYnUrMTv5gAB49AAAAAAAAAAAAAAjmNfRUPz07KkjI5jX0VD89OypLh%2FZDjJ2yhIANRRAAAAAAAAAAAAAAAAAABYGFfV%2BD3u7SnYQ4%2BFfV%2BD3u7SnYQycnfK%2FTthkAHDoAAAAAYNO53KC10jp515GsTS9diHtV1UNHTPqJ3bmONM1Urq63Oa61jp5eK1OCNmfAxO%2FaTYcU3n%2FiPJfjD4uNyqLnVLPUO5GsTQxNiGqAaURERqFOZ39gAPXgAAAAAAAAAAAAAHWsViku027fmylYvHfrcv5U%2FwB4BYrFLdpt0%2FNlMxeO%2FwDNyJy%2FQn0EEVNC2GFiMjYmTWpoRCrmzcfxr9psePfzLMEMdPC2GFiMjYmTWpoRD0MGSgtgAAAAAAAAAAAAAAABHMa%2Biofnp2VJGRzGvoqH56dlSXD%2ByHGTtlCQAaiiAAAASqy4ZoLjaoaqZ0ySP3We5fknAqpsOL3ikbl1Ws2nUIqCc%2BRlr%2FNUdYncPIy1%2FmqOsTuIvc0SejZBgTnyMtf5qjrE7h5GWv8ANUdYncPc0PRsgwJz5GWv81R1idw8jLX%2Bao6xO4e5oejZBgTnyMtf5qjrE7h5GWv81R1idw9zQ9GyDAnPkZa%2FzVHWJ3DyMtf5qjrE7h7mh6NnvhX1fg97u0p2ENegoYrdSMpYN14NmeW6XNeFczZKF53aZWqxqNAAOXoAABgycbEt0W3W1UjdlPPxGcm1eb7nVazadQ8mYiNyjuKLwtdVrSQu%2FwCPA7JVRfxu1rzaDggGpSsVrqFG1ptO5AAduQAAD3pKKprpfB0sD5Xa8k4E966EO3ZMKyVaNqa7dRQrwtjTgc%2F37E%2FkmNPTQ0sKQwRNjYmhrUyQrZOoivxX5TUxTPzKKUeCpXojqyqSP%2ByJM16VOtDhK0xJxoXyrtfIv2yO0hkqTmvb%2BrEY6x%2FHMTDtoRMvEIuhTylwtaJEXKl8Gu1j1Q7AOed%2FL3jXwilXglmSrR1bkXU2VM06UI9X2iutq%2F8AJgVrNUjeFq8%2FeWWYcxr2q1zUc1UyVFTNFJa9RePv5R2xVn6VQdWxWOW7T7p2bKZi8d%2B3kTl%2BhIa3B9JPVMlp3rTxq7OSNqZoqf27DvU9PFSwNhhYjI2Jk1qaiW%2FURx%2FH7cVw%2FPyQQRU0DIYWIyNiZNamo9AZKSyAAAAAAAAAAAAAAAAAAARzGvoqH56dlSRkcxr6Kh%2BenZUlw%2Fshxk7ZQkAGoogAAFg4W9Xqb93aUr4sHC3q9Tfu7SlXqeyE%2BHudgAFBaAAAAAAAAAAAAAAAAAABgrvEVw3wu8itdnFD5tnNpXnUmt7rfELTUTouT0buWfEvAhWxc6an3ZXzW%2FgAC6rAAAEqwxh5Ho24VrM00wxrr%2FuX7HNw5aN867dStzp4cnP%2FALl1NLARMkyTQVOoy6%2FGFjFTf5SGQCisgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABHMa%2Biofnp2VJGRzGvoqH56dlSXD%2ByHGTtlCQAaiiAAAWDhb1epv3dpSviwcLer1N%2B7tKVep7IT4e52AAUFoAAAAAAAAAAAAAAAAAMARXG1VlHTUiL%2BJVkcnu4E%2BqkROziufw19kbnwRMaxOjNfqcY08NdUhSyTu0gAJkYOFdCZrs2g6mHKTxy9wNcmbI18I7m0fzkc2njEy9iNzpNLJb0tlsigy84qbqRdrl093MdEwZMmZmZ3K%2FEajQADx6AAADBpOvVsa5Wur6dFauSp4ROA9iJn6eTMQ3gaG%2Fdr%2FAFCn6xBv3a%2F1Cn6xD3jbwco8t8Ghv3a%2F1Cn6xBv3a%2F1Cn6xBxt4OUeW%2BDQS92tVyS4U%2Ba%2F8A6IbycJ5MTH2RMSyADx6AAAAAAAAAHnPPFTRLLPK2ONNLnLkiAegNDfu1%2FqFP1iDfu1%2FqFP1iHXG3h5yjy3waG%2Fdr%2FUKfrEG%2Fdr%2FUKfrEHG3g5R5b5HMa%2Biofnp2VOpv3a%2F1Cn6xDj4vmiqLLTywyNkjdOmTmrmi8CkmKJi8bcXmJrKGgA01IAAAsHC3q9Tfu7SlfE3w7dKCmsdPFPWQxyN3WbXPRFTjKVupiZp8JsM6skQNDfu1%2FqFP1iDfu1%2FqFP1iFHjbwtco8t8Ghv3a%2F1Cn6xBv3a%2F1Cn6xBxt4OUeW%2BDQ37tf6hT9YhtwTxVMSSwSNkjdoc1c0U8mJj7ImJegAPHoAAAAAAAAYMmAKzu8nhbxWP2zO%2FhcvsaZ6VDt3VTOXXI5f5U8zXrGohnz9gAOngSrBECK%2BrqF0ojY0%2Bq%2FYipNsFsRLTK%2FLhdOv8IhB1E6xylxR%2BSRgAzVwAAAAAYUgOKLb4jdFljblDUcduWhHf1J9%2BcnxzMQW3fK1SRsTOWPjx%2B9NXOnATYb8Lo8leVVdAA01IAAAsDDFy8ftTWvdnNBxH7VTUvR9CvzqYduO911Yr1yim83JyZ6F5lIc1OdP8SY7cbLEMmEMmYugAAAAAAAMEPxncN3NFb2O4Geck966E6OHnJXVVEdJTS1Eq5Mjarl5isamokq6qWol%2FHK5XLychZ6am7cvCHNbUaeQANBUAAB6U8ElTURwRJm%2BRyNb71JXimljosPUlNEnEjla1OXirwmrg23%2BFqZK96cWLiR%2FEuleZPqdHGnoqH56dlSpe%2B8ta%2BE9a6xzKEgAtoAAAAAAAAAAACf4T9X4fif2lIAT%2FAAn6vw%2FE7tKVup7E2HudoAGetgAAAAAAABgyYAqqdMqiVF1Pd9VPg2LizwVzqmZZbmZ%2F1U1zYj6Z8%2FYAD14E4wYudmemyd30Qg5L8ESotPVw58LXtfl70y%2BxX6iP%2FNLh7kqABnLgAAAAAGNRkwBX%2BJrb4hdXPY3KKo47diLrTp4ec45YeIratxtT2sbnLFx4%2BVU0pzoV4aWC%2FKn%2FAGFPJXjYABOiAABYOGrlvham7t2c0PEk5di86HXK9w1ct77qxHuyhn82%2Fk2L0%2FUsJDMzU4XXcduVWQAQpAAADBk%2BJHtjY571ya1FVVXUgEZxncNxDHQMXhk48nwpoTnX6EPNq5VrrhcJqp3%2FAKO4qbG6k6DVNTFThSIUb25W2AAlcBlrXPcjGJunOXJETWpg7uErf41c1qXpnHTJuve5dH3U4vbjWZdVjlOkvtVC23W6GlbpY3jLtculek5ONfRUPz07KkiI7jX0VD89OypnYp3kiVy8apKEgA1FEAAAAAAAAAAAn%2BE%2FV%2BH4ndpSAE%2Fwn6vw%2FE%2FtKVup7E2HudoAGetgAAAAAAABgyAK6xLD4G%2F1SZcD1R6c6HLJNjam3NXTVKJwPYrF96Lmn1IyamKd0iVG8atIACVwHcwjVJT3lInLk2dis504U%2B5wz7gmfTzxzxrk%2BNyOb70OL15VmHVZ1O1qmTwpKmOspIqmJc2StRyHuZK%2BAAAAAAAAwV7iS3b33V6sblDP5xmxNqdP1LCORiS274Wp%2B4bnND5yPl2pzoTYb8Lo8leVVfAA01IAAAsPDty3xtbHPdnNF5uTlVNC86FeHYwzcfELq1j3ZRVGTHci6l6fqQZ6cqf4lxW42WCDCGTNXAAAYI%2Fi%2B4eLW5KRjspKlcl5GJp%2ByEgXgK3vlw3yus0yLnG1dxH8Kd%2FCpPgpyvvwiy21VzwAaSmAAAWNYLfvdaoonJlK%2FjyfEurm0EPw3b%2FH7szdpnFD5x%2FLloTp%2BhYRS6m%2F%2FwArOGv9ZI5jX0VD89OypIyOY19FQ%2FPTsqQYf2QlydsoSADUUQAACT2fC9JcbXFVyzzNfJnmjcsuBVTYRgsHC3q9Tfu7SlfqLTWu4S4oiZ%2BWl5E0PtNR0t7h5E0PtNR0t7iSgp%2Btk8rPp18I15E0PtNR0t7h5E0PtNR0t7iSgetk8np18I15E0PtNR0t7jtW2gjttEylic5zGKq5u0rmuZtg5te1viZexWI%2BgAHDoAAAAAAAAAAHFxTR%2BNWWVzUzfAqSJ7k0%2FwAZkALXe1r2ua5M2uTJU2oVjcaN1vuE1K7%2FAM3cVdrdKL0F3pbfE1Vs1fnbWABcVwAASnB91RjnW2Z3A5VdCq7dbfv0kwKnY90b2vY5WuaqK1yaUUsGwXtl2pcnqjamNPON2%2F3JyFHqMWp5QtYr7%2FGXXBhDJUTgAAAAAYMgCusRW7e66vRjcopvOR8melOZTllgYmt3j9qc5jc5YPOM5U1p0FfmngvzopZK8bAAJkYAALFw%2Fcd8rXHI5c5Y%2BJJ70186cJ1CAYWuXiN0SJ7soqjJjtiO%2FpX7c5PjLzU4XXcduVWQDBEkcfE9w8RtL2sdlLP5tnJtXo%2BpX518TXDx67Pa12cVP5tvKuten6HINLBTjT%2FVPJblYABOiADestAtyukNOqcTPdSfCmnu5zyZiI3L2I3Okwwtb%2FErU2R7cpajju5E1J0fU7ZhEREyRMkQyZNrTaZmV%2BsajQRzGvoqH56dlSRkcxr6Kh%2BenZU7w%2Fshzk7ZQkAGoogAAFg4W9Xqb93aUr4sHC3q9Tfu7SlXqeyE%2BHudgAFBaAAAAAAAAAAAAAAAAAABgjGMrZ4SBlwjbxouLJl%2BXUvMv1JQfEsbJo3RyNRzHorXIutDulppbbm1eUaVSDdu9tfaq99O7NWfijd%2BZv8AvAaRqxMTG4UZjU6kAB68D0pqmakqGTwSLHIxc0ch5g8ep%2FZMRQXNqRSqkNUicLFXgdyt7jslToqouaLkqaFQ79sxbV0iJHVt8ajTWq5PTn185TydPP3RYpm%2Flk6By6PEVsrURGVLY3r%2FAES8Vf54DpNcjkzaqKm1CpNZr9p4mJ%2Bn0DAzPHrINGrvNvokXw9XGjk%2FpRd07oQ1LZiSmulc%2BmijezJu6Y5%2F9e3g1HXC2t6%2BHPKN6djIrvEVtS23R7WJlDLx4%2BTanMpYpxcT27x%2B1Oexuc0HHZyprTo%2BhJgvwu5yV5VQAAGmpAAAc%2BXKWNYbjvna45nL51vEl%2BJNfPpK5O3hS4%2BJXRIHuyiqcmryO1L9ucgz05U34S4rasnxzb5cN7bVLMi5SKm4j%2BJf9z5jokGxdcPGbilKx2cdMmS8r109CcBSxU53iFnJbjVwP5ABqKIAABNsHW%2FwFA6se3j1C8XkYmjpXhIjb6N9wroaVn%2Fo7JV2JrXoLOijZDG2NibljERrU2IhU6m%2Bo4wnw1%2BdvsAFFaCOY19FQ%2FPTsqSMjmNfRUPz07KkuH9kOMnbKEgA1FEAAAsHC3q9Tfu7SlfFg4W9Xqb93aUq9T2Qnw9zsAAoLQAAAAAAAAAAAAAAAAAABgyAOZfLQy7UKx8DZmcaJ66l2LyKV3LFJDK%2BKVisexcnNXUpaxwcR2BLjH4zTNRKpiaNHhE2e%2FYWcGXj%2BM%2FSHJj5fMIKDLmuY5WuarXNXJUVMlRTBoKgAAAAAH3FPND%2FANU0kfwPVPofAD1tpdrkiZeP1PWqeMlXUzf9tTNJ8UiqeQOeMR%2FDcmjQe1HVSUVZFUxfiicjstu1Og8QezG%2Fgj4WpTVEdTTxzxLmyRqOavIp6ZcBF8G3HdwSW%2BR3Gj48fwrpTmX6koMq9eFpherblG1c363b23WSJqZRSceP3Lq5lOaTzFVt8dtizRtzlpuOmWtv9SffmIGaGG%2FOipkrxsAAmRgRVRc0VUVNCpqAAn1NfmOw2txeqLJE3cvbtfoTp4F5yBPe6R7pHu3TnKrnLtVT6SaRIHQI9fBucjlbqVU0L%2FJ8EWPFFN%2F9SXvNtAAJUYAelPA%2BqqI4IkzfI5Gt5zx7CVYLt%2BUctwenC7zcfuTSvTwcxKjxo6aOjpYqaJOJG1Gp3nuZWS3O0yvUrxroABw6COY19FQ%2FPTsqSMjmNfRUPz07KkuH9kOMnbKEgA1FEAAAsHC3q9Tfu7SlfFg4W9Xqb93aUq9T2Qnw9zsAAoLQAAAAAAAAAAAAAAAAAAAAAGDIA4V%2Bw5HcmrUU%2BUdUiadUnIvLykHnglppnQzxujkZpa5OFC1DRudopLrFuKhnHROJI3gc3%2FdhZxZ5p8T9Ib4uXzCtQdS6YerbYqvVvhoE0SsTR701HLL1bRaNwqzExOpAAdPAAAAAAAAGxb6x9vroqpmfm3Zqm1NadBZ0MrJoWSxu3THtRzV2opVJNMHXHw1G%2Bhe7jwcLOVi9y%2FUqdTTcck%2BG2p0kioioqKmaKVve7ctsuksCJlG7jx%2FCurm0Fkajg4stvjlt8YjbnLTcbg1t1p9%2BYgwX43%2F1LlruqCgA0lMAAAAAAAAJNg23%2BFqJK97eLFxI8%2FzLpXmT6kaYxz3tYxN05yojUTWqlmWqhbbrdFSt0sbxl2uXSvSVuovxrrymxV3bbbMgGetgAAEcxr6Kh%2BenZUkZHMa%2Biofnp2VJcP7IcZO2UJABqKIAABYOFvV6m%2Fd2lK%2BLBwt6vU37u0pV6nshPh7nYABQWgAAAAAAAAAAAAAAAAAAAAAAAAwZAHyqIuk41xwtb65VkjatNKv9UacC%2B9ug7YOq2ms7h5MRP2r%2Btwtc6TNY40qWJ%2FVFp6F4TkPY%2BJ24kY5jk1OTJf5LXPGanhqG7maFkqbHtRSzXqZjuhDOGP4qwFgzYXtE2a%2BK%2BDVdcblaaj8FW934J6hn7kX7EsdTT%2Bo5w2QkEy8iKX22f%2FFp9MwVQp%2BOpqHe7cp9j33GN56VkLCJmuScK7E0k%2BhwnaIvxQvl%2BZIq%2FQ6VPQUlImVPTRRcrWIi9JxPVV%2FkOowz%2FUCosPXOtyVlMsbF%2Frl4qd5KLLhmO1zJUyVDpZ0RU4vFamfJrO8CC%2Be9o0mrirX5YTQYciORWqmaLwKi6z6MECRWt5t62y5y0%2BXm891Gu1q6OjRzGiTnFtt8atyVUbc5abhXLWzX3kGNTDfnXalkrxsAAlRgAAAADvYRt%2FjVyWpenm6ZM05Xro6OFegnSHNsFv3utUUTkyldx5PiXVzaDpmXlvzvtex141AAROwAACOY19FQ%2FPTsqSM593tUd3pWwSSujRr0eitRF1Kmv3neO0VvEy5tG6zEK2BMfIim9tm%2FxaPIim9tm%2FxaX%2FcY%2FKr6V0OBMfIim9tm%2FwAWjyIpvbZv8Wj3GPyeldDiwcLer1N%2B7tKc%2FwAiKb22b%2FFp3bbQsttDHSMe57Y8%2BM7Sua5kGfLW9dQlxUtWdy2wAVE4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADBkADAMgAYMgAAAAAA%2BXtR7Va5EVFTJUXWVrd7etsuUtNw7hF3Ua7Wro7uYssj2L7d4zQJWRt85T%2FAIstbF09GnpJ8F%2BNteUWWu6oQADSUwAADrYat%2Fj93Yrm5xQecfy7E6fockn2Frf4laWyPblLUecdyJqTo%2BpBmvxokxV5WdpDJhDJmroAAAAAGDIAAAAAABgyAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB8vY17HMeiOa5MlRdaH0YArK60DrbcZaVc9y1c2Ltaug1Ca4wt3h6JtaxvHp%2BB3Kxe5fuQo1MN%2BdNqN68baAASuG%2FZKDfK6QwKmcaLu5PhTv0c5ZKJkmScBHcH2%2FwAXt7qx6ceoXi8jE0dK8PQSIzc9%2BV9eFzFXVQyAQJQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwZAHxJG2WN0b0RzXIqKi60KzudC63XCaldoYvFXa1dClnEaxjbvDUjK6NvHg4H5a2L3L9Sx09%2BNteUWWu42hhs26jdcK%2BGlb%2FAOjslXY3WvQaxLsGW%2Fcxy3B7eF%2Fm4%2Fcmlen6F3LfhWZVqV5W0lEUbYo2xsbuWMRGtTYiH2YMmUvAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADB8SxMmifFI3dMe1WuRdaKehgCtKi1zw3hba1M3rIjWLtRdC9BYtJTMo6WKmiTJkbUah8Pt9NJXx1zmZzxsVjXZ6l%2F3%2BTaJsmWbxCOlOOwAEKQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAH%2F%2FZ&logoColor=white&logoSize=auto&labelColor=white&color=%23AF001E
" style="width: auto; height: 25px;"></a>   <a href="https://github.com/vita-epfl/MotionMap"><img alt="GitHub" src="https://img.shields.io/badge/GitHub-Code%20(link)-gray?style=flat&logo=GitHub&logoColor=black&logoSize=auto&labelColor=white
" style="width: auto; height: 25px;"></a> <a href="https://github.com/vita-epfl/MotionMap"><img alt="Presentation" src="https://img.shields.io/badge/Presentation%20(link)%20-%20white?style=flat&logo=googleslides&logoColor=white&logoSize=auto&color=%23C78C06
" style="width: auto; height: 25px;"></a>

<hr style="border: 1px solid gray;">
<br><br>

## Problem Statement

Predicting human motion from observed skeletal poses seems straightforward, yet beneath the surface lies an intriguing complexity: for any given pose sequence, an infinite number of possible future motions exist. This inherent multimodality challenges existing forecasting models, which typically rely on oversampling a large number of predictions. However, no matter how many futures are generated, these methods inevitably fail to cover all realistic possibilities, often missing critical or rare motions essential for real-world applications.

{% include elements/highlight.html text="<em>To overcome this limitation, it becomes essential to rethink human pose forecasting. Rather than aiming to capture endless hypothetical futures, a more practical approach is to explicitly learn and represent the realistic transitions observed in available data. By grounding multimodal predictions firmly within observed transitions, the forecasting task transitions from an ill-posed to a well-posed challenge.</em>" %} But first, what does multimodality in human pose forecasting mean?

## Multimodality in Human Pose Forecasting

Before diving deeper into our methodology, let's first clarify what we mean by "multimodality" in human pose forecasting. In simple terms, multimodality refers to the presence of multiple distinct yet plausible future motions from a single observed sequence. Imagine seeing a person standing still—this single pose could naturally lead to multiple realistic futures, such as walking forward, turning around, or even sitting down. Each of these actions represents a different "mode." Importantly, each mode comprises a set of likely and coherent motions logically connected to the observed pose. By recognizing these diverse yet realistic futures, multimodal forecasting models can offer richer and more informative predictions. Formally, we define multimodality as <br>

<div style="text-align: center;">{% include elements/highlight.html text="<em>Multimodality in human pose forecasting refers to a diverse yet realistic set of future actions with a
logical transition from an observed pose sequence.</em>" %}</div> <br>

But how exactly can we efficiently encode multimodality? And how do we effectively distinguish between likely and unlikely future motions? Answering these questions could lead to more robust, realistic, and practical pose forecasting solutions.

<br><br>

## MotionMap

<img src="https://github.com/deep-regression/deep-regression.github.io/blob/master/files/images/motionmap/pullfig.png?raw=true" alt="MotionMap Idea" style="width: 100%; height: auto;">

That's exactly where the MotionMap representation comes in, marking a new way of deep regression. We can think of MotionMap as a kind of visual map, specifically a heatmap that clearly shows all the different paths human motion can realistically take from any observed pose. On this map, distinct peaks mark the most probable future movements, directly learning from the actual transitions observed in the training data. Unlike traditional methods that need countless random predictions, MotionMap naturally captures multiple scenarios at once, even highlighting the rare yet crucial motions we can't afford to overlook. With this representation, we predict the explicit distribution over different future sequences for an observation. Why is this important? First, we know the exact number of future motions per sample, allowing us to be much more **efficient** with our predictions. Second, we also know which future is more likely since the intensity of different peaks can be treated as a measure of confidence. Third, MotionMap can be used in tandem with metadata such as action labels for **controllable** human pose forecasting. For instance, we can map different confidences to different actions, allowing the user to generate motion sequences based on the action and confidence. Fourth, MotionMap predicts the confidence of the mode, and the uncertainty in the motion conditioned on the mode. As a result, we get accurate uncertainty estimates allowing for **safe** deployment. Finally, our quantitative results show that MotionMap is **robust** and is state-of-the-art in human pose forecasting.

But how do we learn MotionMap for each sample, and how exactly do we use MotionMap for predicting multimodality given an observation? We answer this in the next section.

<br><br>

## Methodology

<img src="https://github.com/deep-regression/deep-regression.github.io/blob/master/files/images/motionmap/schematic.png?raw=true" alt="Overview" style="width: 100%; height: auto;">

Let's break down how MotionMap works. First, we use an autoencoder which is a model that learns how to compress and then reconstruct human motion sequences. This autoencoder takes as input the observation and one of the many multimodal ground truths, with the goal of compressining and reconstructing the entire pose sequence (input + multimodal ground truth). During training, the autoencoder captures key patterns from past (observed) and future (predicted) skeletal poses. *Essentially, it becomes good at recognizing realistic motion transitions by seeing many examples of how poses evolve.*

The catch is, during actual prediction, we won't know the future poses. So how do we still manage to predict realistic future movements? Our key intuition is that even if we don't have the future pose, what we need is a latent that describes the future pose. This is where our MotionMap comes to the rescue. With MotionMap, we represent an observation's multimodal ground truth through heatmaps. The way we construct this is by first encoding each multimodal ground truth pose sequence into a vector using the autoencoder. Next, we use t-SNE, a popular dimensionality reduction technique, to embed each encoding into two dimensions. It is these two dimensional encodings that we represent through local maxima in MotionMap. We train a model that predicts the MotionMap directly from the observed poses. On the predicted MotionMap, the maxima represent the likeliest future motions corresponding to a given observation. However, how do we go from maxima to the predicted future pose sequence?

To convert these heatmaps into actual predicted movements, we use a codebook. The codebook is a dictionary / map that links each maxima on the heatmap to its corresponding embedding (which was reduced to 2D earlier). By looking up the heatmap's peaks in this dictionary, we can quickly obtain the latent embedding for the pose sequence. As a result, we have the final piece in the jigsaw puzzle to make the autoencoder work at test time: the missing *future* latent obtained from MotionMap.

<br><br>

## Experiments

We evaluate different state-of-the-art methods under our well-posed setup, which as described involves learning to translate different transitions in the training dataset to *unseen test samples*. To do this, we select a subset of test samples with multimodal ground truth in the training dataset. Futhermore, we focus on prediction efficiency, and evaluate different methods with less than 10 samples. Our results are shown below.

<img src="https://github.com/deep-regression/deep-regression.github.io/blob/master/files/images/motionmap/results.png?raw=true" alt="Results" style="width: 100%; height: auto;">

We make two key observations from our results. First, we see that methods with high diversity may not necessarily be accurate. This is because many of the predicted transitions are not realistic given the observed pose sequence. Second, we observe that MotionMap has a higher accuracy in the multimodal metrics, especially when compared to state-of-the-art diffusion based methods. This is because MotionMap explicitly learns diverse transitions, and is able to translate them to test samples. In fact, our additional results in the appendix show that the ability to recall transitions does not come at the cost of predicting generic motions for unseen samples. Our results show that heatmaps and codebooks outperform diffusion and repeated sampling.

Apart from the quantitative evaluation, we also provide extensive qualititative analysis centred around uncertainty, controllability, ranking and diversity. Our first analysis is centred around comparing different methods through the MotionMap representation.

<img src="https://github.com/deep-regression/deep-regression.github.io/blob/master/files/images/motionmap/sampling.png?raw=true" alt="Sampling Comparison" style="width: 100%; height: auto;">

This comparison compares multimodal predictions by different methods through MotionMap. We do this by embedding multimodal predictions for three different samples *(a, b, c)* on their respective ground truth MotionMap. We confirm our quantitative evaluation: methods that focus on high diversity do so at the cost of realism, predicting transitions that are non-existent in the dataset. Although diffusion based methods are much more realistic, they fail to capture all different transitions, including some that are rare. In constrast, MotionMap, the method successfully captures diversity and realism.

<img src="https://github.com/deep-regression/deep-regression.github.io/blob/master/files/images/motionmap/ranking.png?raw=true" alt="Sampling Comparison" style="width: 100%; height: auto;">

MotionMap predicts the confidence for different modes through local maxima in the heatmap. Modes with higher confidence have peaks of higher intensity in the predicted MotionMap. This allows us to rank different predictions based on the confidence as shown. Predictions closed to the ground truth motion have higher confidence, with rare modes and potentially unnatural motions having lower intensity peaks in MotionMap.

<img src="https://github.com/deep-regression/deep-regression.github.io/blob/master/files/images/motionmap/uncertainty.png?raw=true" alt="Sampling Comparison" style="width: 100%; height: auto;">

We also visualize the uncertainty learnt by MotionMap. Not only do we introduce heteroscedastic modelling, by introducing the concept of mode and forecast, we can also improve the semantics behind the learnt uncertainty. Not only does MotionMap predict the confidence of each mode, the uncertainty network predicts the uncertainty for the pose sequence conditioned on the mode. This is important since without conditioning on the mode, previous methods counterintuitively showed that homoscedastic modelling is better than heteroscedastic modelling. Intuitively, since the predicted motion is multimodal, conditioning on a mode prevents the learnt uncertainty from averaging across all the modes.

In this figure, we note two observations. First, joints with a higher degree of mobility have larger uncertainty estimates. This is expected since mobile joints have a larger range of motion. Second, we highlight the role of the nose as a joint that indicates the orientation of the person. We note that the uncertainty for the nose is higher for the pose sequence which involves a faster turn in comparison to the motion with a slower turn.

<img src="https://github.com/deep-regression/deep-regression.github.io/blob/master/files/images/motionmap/controllability.png?raw=true" alt="Controllability" style="width: 100%; height: auto;">

MotionMap can be used in tandem with metadata to control the generated motion sequence. For datasets such as Human 3.6M which come with action labels, we can establish a duality between the 2-D embeddings of pose sequences and their action labels. Therefore, predicting the MotionMap also corresponds to predicting the likelihood of different actions as the forecasted motion for the observed pose sequence. Consequently, downstream tasks can control the generation of different futures by choosing from within the likeliest actions, which we show in the figure above. This allows us to incorporate user preference in the pose forecasting pipeline.

<img src="https://github.com/deep-regression/deep-regression.github.io/blob/master/files/images/motionmap/diversity.png?raw=true" alt="Controllability" style="width: 100%; height: auto;">

We conclude our qualitative study by visualizing the diversity of the predicted motions. By using the same decoder as state-of-the-art methods like BeLFusion, we note that MotionMap predicts diverse yet realistic future pose sequences. 

<br><br>

## Wrapping Up: A Smarter Approach to Human Pose Forecasting

In this work, we tackled the challenge of making human pose forecasting well-posed and more efficient. Our proposed representation, MotionMap, explicitly encodes multiple possible future motions while also quantifying their confidence. This allows us to distinguish more likely movements from less probable ones, also offering a structured approach to uncertainty in motion prediction. By modeling the spread of future motions directly, MotionMap eliminates the need for excessive random sampling, making it both sample-efficient and robust. We demonstrated how this approach leads to more diverse yet realistic motion predictions, ultimately improving reliability in real-world applications. Beyond accuracy, MotionMap also paves the way for controllable pose forecasting and practical uncertainty estimation—two aspects that could be highly valuable in domains like robotics, animation, and autonomous systems. Through comprehensive analysis, we showcased the effectiveness of MotionMap in enhancing both the safety and reliability of human motion forecasting. {% include elements/highlight.html text="<em>Our takeway? MotionMap shows that heatmaps and codebooks can outperform diffusion and repeated sampling in human pose forecasting!</em>" %} Looking ahead, we believe this paradigm shift can inspire future research in making motion prediction more structured, interpretable, and useful in real-world scenarios.

<br><br>

## Citation

If our work is useful, please consider citing the accompanying paper and starring our code on GitHub!

```
@inproceedings{
hosseininejad2025motionmap,
title={MotionMap: Representing Multimodality in Human Pose Forecasting},
author={Reyhaneh Hosseininejad and Megh Shukla and Saeed Saadatnejad and Mathieu Salzmann and Alexandre Alahi},
booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2025}
}
```
<br>